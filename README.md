è¿™æ˜¯ä¸€ä»½ä¸ºä½ å®šåˆ¶çš„ `README.md`ï¼Œå®ƒå®Œå…¨é€‚é…ä½ æ•´ç†å¥½çš„ `miniGR-CF` é¡¹ç›®ç»“æ„ã€‚

è¿™ä»½æ–‡æ¡£ä¸¥æ ¼éµå¾ªäº†ä½ çš„è¦æ±‚ï¼š

1.  **ä»é›¶å¼€å§‹**ï¼šæ¶µç›–ç¯å¢ƒé…ç½®ã€æ•°æ®ä¸‹è½½ã€é¢„å¤„ç†åˆ°è®­ç»ƒçš„å…¨æµç¨‹ã€‚
2.  **å•è¡Œå‘½ä»¤**ï¼šæ‰€æœ‰çš„ Shell å‘½ä»¤éƒ½å·²åˆå¹¶ä¸ºä¸€è¡Œï¼Œæ–¹ä¾¿ç›´æ¥å¤åˆ¶è¿è¡Œï¼Œé¿å…æ¢è¡Œç¬¦å¸¦æ¥çš„å…¼å®¹æ€§é—®é¢˜ã€‚
3.  **é€»è¾‘å¯¹é½**ï¼šå‚æ•°é…ç½®ï¼ˆå¦‚æ—¶é—´çª—å£ã€æ•°æ®é‡ã€Hintæ³¨å…¥ï¼‰å‡å¯¹åº”æˆ‘ä»¬æœ€ç»ˆç¡®å®šçš„â€œå¤åˆ»+å¢å¼ºâ€æ–¹æ¡ˆã€‚

ä½ å¯ä»¥ç›´æ¥å°†ä¸‹é¢çš„å†…å®¹ä¿å­˜ä¸º `README.md`ã€‚

-----

# miniGR-CF: Collaborative-Enhanced Generative Recommendation

**miniGR-CF** æ˜¯ä¸€ä¸ªå¢å¼ºç‰ˆçš„ç”Ÿæˆå¼æ¨èç³»ç»Ÿæ¡†æ¶ã€‚å®ƒåœ¨åŸç‰ˆ MiniOneRec (Qwen-0.5B) çš„åŸºç¡€ä¸Šï¼Œåˆ›æ–°æ€§åœ°å¼•å…¥äº† **LightGCN ååŒæç¤º (Collaborative Hints)** æœºåˆ¶ã€‚é€šè¿‡å°†ååŒä¿¡å·ä½œä¸º Prompt æ³¨å…¥ï¼Œå¹¶é…åˆ **åŠ¨æ€ Hint Dropout** å’Œ **é˜²æ³„éœ²æ¸…æ´—** ç­–ç•¥ï¼Œæœ¬é¡¹ç›®åœ¨ä¿æŒè¯­ä¹‰æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨èçš„æ’åºç²¾åº¦ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
miniGR-CF/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # [éœ€æ‰‹åŠ¨ä¸‹è½½] å­˜æ”¾ Amazon åŸå§‹ .json.gz æ–‡ä»¶
â”‚   â””â”€â”€ processed/           # [è‡ªåŠ¨ç”Ÿæˆ] ä¸­é—´æ•°æ® (.inter, .json, .npy)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Qwen2.5-0.5B/        # [éœ€ä¸‹è½½] é¢„è®­ç»ƒæ¨¡å‹æƒé‡
â”œâ”€â”€ src/                     # æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ process_raw.py       # æ•°æ®æ¸…æ´— (å« 2017-2018 æ—¶é—´è¿‡æ»¤)
â”‚   â”œâ”€â”€ generate_sids.py     # è¯­ä¹‰ ID ç”Ÿæˆ (RQ-VAE/KMeans)
â”‚   â”œâ”€â”€ train_lightgcn.py    # ååŒä¿¡å·æå–
â”‚   â”œâ”€â”€ generate_hints.py    # Hint å­—å…¸ç”Ÿæˆ
â”‚   â”œâ”€â”€ convert_dataset.py   # SFT æ•°æ®è½¬æ¢ (ä¸»ä»»åŠ¡ç²¾ç®€)
â”‚   â”œâ”€â”€ dataset.py           # åŠ¨æ€ Dataset (å« Dropout/å¤šä»»åŠ¡)
â”‚   â”œâ”€â”€ train.py             # SFT è®­ç»ƒå…¥å£ (5ä»»åŠ¡æ··åˆ)
â”‚   â”œâ”€â”€ evaluate.py          # æ¨ç†ç”Ÿæˆ
â”‚   â””â”€â”€ metrics.py           # æŒ‡æ ‡è®¡ç®—
â””â”€â”€ output/                  # è®­ç»ƒæ—¥å¿—ä¸æ¨¡å‹ä¿å­˜
```

## ğŸ› ï¸ 1. ç¯å¢ƒå‡†å¤‡

æ¨èä½¿ç”¨ Conda ç¯å¢ƒï¼ˆPython 3.10+ï¼‰ï¼š

```bash
conda create -n minigr python=3.10 -y && conda activate minigr
pip install torch>=2.0.0 transformers accelerate pandas numpy scipy scikit-learn fire tqdm
conda install -c pytorch faiss-gpu
```

## ğŸ“¥ 2. èµ„æºå‡†å¤‡

### 2.1 ä¸‹è½½æ¨¡å‹

è¯·ä¸‹è½½ Qwen2.5-0.5B æ¨¡å‹è‡³ `models/` ç›®å½•ï¼š

```bash
huggingface-cli download --repo-type model "Qwen/Qwen2.5-0.5B" --local-dir "models/Qwen2.5-0.5B" --local-dir-use-symlinks False
```

### 2.2 ä¸‹è½½æ•°æ®

æœ¬é¡¹ç›®é»˜è®¤ä½¿ç”¨ **Industrial and Scientific** æ•°æ®é›†ã€‚è¯·ä» [UCSD Amazon Data](https://nijianmo.github.io/amazon/index.html) ä¸‹è½½ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶å¹¶æ”¾å…¥ `data/raw/`ï¼š

  * `Industrial_and_Scientific_5.json.gz`
  * `meta_Industrial_and_Scientific.json.gz`

-----

## ğŸš€ 3. è¿è¡Œå…¨æµç¨‹ (Step-by-Step Pipeline)

è¯·æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚æ‰€æœ‰å‘½ä»¤å‡å·²è®¾è®¡ä¸ºå•è¡Œæ‰§è¡Œã€‚

### Step 1: æ•°æ®æ¸…æ´— (Data Processing)

è§£æåŸå§‹æ•°æ®ï¼Œæ‰§è¡Œ K-Core è¿‡æ»¤ï¼Œå¹¶æŒ‰ **2017.10-2018.11** æ—¶é—´çª—å£åˆ‡åˆ†æ•°æ®ã€‚

```bash
python src/process_raw.py --raw_dir ./data/raw --out_dir ./data/processed --cat Industrial_and_Scientific --st_year 2017 --st_month 10 --ed_year 2018 --ed_month 11
```

### Step 2: ç”Ÿæˆè¯­ä¹‰ ID (Semantic IDs)

ä½¿ç”¨ Qwen æå–å•†å“æ ‡é¢˜å‘é‡ï¼Œå¹¶é€šè¿‡å±‚çº§èšç±»ç”Ÿæˆ 3 å±‚è¯­ä¹‰ IDã€‚

```bash
python src/generate_sids.py --data_dir ./data/processed --model_path ./models/Qwen2.5-0.5B --cat Industrial_and_Scientific
```

### Step 3: æå–ååŒä¿¡å· (Collaborative Signals)

è®­ç»ƒ LightGCN æ¨¡å‹ä»¥æ•è·ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶å¯¼å‡ºç‰©å“ååŒå‘é‡ã€‚

```bash
python src/train_lightgcn.py --data_dir ./data/processed --cat Industrial_and_Scientific
```

### Step 4: ç”ŸæˆååŒæç¤º (Generate Hints)

åŸºäº LightGCN å‘é‡æ£€ç´¢æ¯ä¸ªç‰©å“çš„ Top-K äº’è¡¥é‚»å±…ï¼Œç”Ÿæˆæç¤ºå­—å…¸ã€‚

```bash
python src/generate_hints.py --data_dir ./data/processed --cat Industrial_and_Scientific
```

### Step 5: å‡†å¤‡ SFT æ•°æ® (Prepare Data)

ç”Ÿæˆç²¾ç®€ç‰ˆï¼ˆKeep Longest Onlyï¼‰çš„ä¸»ä»»åŠ¡è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨ç”Ÿæˆæ—¶æ³¨å…¥ Hintã€‚

```bash
python src/convert_dataset.py --data_dir ./data/processed --cat Industrial_and_Scientific --out_dir ./data/sft_ready --hints_file ./data/processed/hints.json --keep_longest_only
```

### Step 6: SFT è®­ç»ƒ (Training)

å¯åŠ¨å¤šä»»åŠ¡æ··åˆè®­ç»ƒã€‚

  * **æ•°æ®æ„æˆ**ï¼šä¸»ä»»åŠ¡ (5760æ¡) + 4ä¸ªè¾…åŠ©ä»»åŠ¡ (å„é‡‡æ ·5760æ¡) â‰ˆ 2.88ä¸‡æ¡æ•°æ®ã€‚
  * **æœºåˆ¶**ï¼šä¸»ä»»åŠ¡å¯ç”¨ **Hint Dropout (p=0.3)**ï¼Œè®­ç»ƒ 10 Epochs (çº¦ 1.8ä¸‡æ­¥)ã€‚

<!-- end list -->

```bash
python src/train.py --out_dir ./output/sft_final --batch_size 128 --micro_batch_size 16 --epochs 10 --dropout 0.3
```

### Step 7: è¯„ä¼° (Evaluation)

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œ Beam Search æ¨ç†ï¼Œå¹¶è®¡ç®— HR@K å’Œ NDCG@Kã€‚
*(æ³¨ï¼šæ¨ç†æ—¶è‡ªåŠ¨ä¿ç•™å®Œæ•´ Hint)*

**ç”Ÿæˆé¢„æµ‹ç»“æœï¼š**

```bash
python src/evaluate.py --model_path ./output/sft_final/final_checkpoint --data_dir ./data/processed --output_file ./output/eval_result.json --num_beams 20
```

**è®¡ç®—æŒ‡æ ‡ï¼š**

```bash
python src/metrics.py --file ./output/eval_result.json
```

-----

## ğŸ“Š å®éªŒå¯¹ç…§

| Experiment | Configuration | Hint Strategy |
| :--- | :--- | :--- |
| **Baseline** | åŸç‰ˆå¤ç° | æ—  Hint |
| **miniGR-CF** | **LightGCN å¢å¼º** | **Train: Dropout(0.3) & Clean Target / Test: Full Hint** |

*æ³¨ï¼šæœ¬é¡¹ç›®é€šè¿‡ `dataset.py` å®ç°äº†åŠ¨æ€é˜²æ³„éœ²é€»è¾‘ï¼Œè®­ç»ƒæ—¶ä¼šè‡ªåŠ¨å‰”é™¤ Hint ä¸­çš„ Target Itemï¼Œé˜²æ­¢æ ‡ç­¾æ³„éœ²ã€‚*