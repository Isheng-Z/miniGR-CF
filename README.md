
-----

# miniGR-CF: Collaborative-Enhanced Generative Recommendation

**miniGR-CF** æ˜¯ä¸€ä¸ªå¢å¼ºç‰ˆçš„ç”Ÿæˆå¼æ¨èç³»ç»Ÿæ¡†æ¶ã€‚å®ƒåœ¨åŸç‰ˆ MiniOneRec (Qwen-0.5B) çš„åŸºç¡€ä¸Šï¼Œåˆ›æ–°æ€§åœ°å¼•å…¥äº† **LightGCN ååŒæç¤º (Collaborative Hints)** æœºåˆ¶ã€‚é€šè¿‡å°†ååŒä¿¡å·ä½œä¸º Prompt æ³¨å…¥ï¼Œå¹¶é…åˆ **åŠ¨æ€ Hint Dropout** å’Œ **é˜²æ³„éœ²æ¸…æ´—** ç­–ç•¥ï¼Œæœ¬é¡¹ç›®åœ¨ä¿æŒè¯­ä¹‰æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨èçš„æ’åºç²¾åº¦ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
miniGR-CF/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # [éœ€æ‰‹åŠ¨ä¸‹è½½] å­˜æ”¾ Amazon åŸå§‹ .json.gz æ–‡ä»¶
â”‚   â””â”€â”€ processed/           # [è‡ªåŠ¨ç”Ÿæˆ] ä¸­é—´æ•°æ® (.inter, .json, .npy)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Qwen2.5-0.5B/        # [éœ€ä¸‹è½½] é¢„è®­ç»ƒæ¨¡å‹æƒé‡
â”œâ”€â”€ src/                     # æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ process_raw.py       # æ•°æ®æ¸…æ´— (å« 2017-2018 æ—¶é—´è¿‡æ»¤)
â”‚   â”œâ”€â”€ generate_sids.py     # è¯­ä¹‰ ID ç”Ÿæˆ (RQ-VAE/KMeans)
â”‚   â”œâ”€â”€ train_lightgcn.py    # ååŒä¿¡å·æå–
â”‚   â”œâ”€â”€ generate_hints.py    # Hint å­—å…¸ç”Ÿæˆ
â”‚   â”œâ”€â”€ convert_dataset.py   # SFT æ•°æ®è½¬æ¢ (ä¸»ä»»åŠ¡ç²¾ç®€)
â”‚   â”œâ”€â”€ dataset.py           # åŠ¨æ€ Dataset (å« Dropout/å¤šä»»åŠ¡)
â”‚   â”œâ”€â”€ train.py             # SFT è®­ç»ƒå…¥å£ (5ä»»åŠ¡æ··åˆ)
â”‚   â”œâ”€â”€ evaluate.py          # æ¨ç†ç”Ÿæˆ
â”‚   â””â”€â”€ metrics.py           # æŒ‡æ ‡è®¡ç®—
â””â”€â”€ output/                  # è®­ç»ƒæ—¥å¿—ä¸æ¨¡å‹ä¿å­˜
```

## ğŸ› ï¸ 1. ç¯å¢ƒå‡†å¤‡

æ¨èä½¿ç”¨ Conda ç¯å¢ƒï¼ˆPython 3.10+ï¼‰ï¼š

```bash
conda create -n minigr python=3.10 -y && conda activate minigr
pip install torch>=2.0.0 transformers accelerate pandas numpy scipy scikit-learn fire tqdm
conda install -c pytorch faiss-gpu
```

## ğŸ“¥ 2. èµ„æºå‡†å¤‡

### 2.1 ä¸‹è½½æ¨¡å‹

è¯·ä¸‹è½½ Qwen2.5-0.5B æ¨¡å‹è‡³ `models/` ç›®å½•ï¼š

```bash
huggingface-cli download --repo-type model "Qwen/Qwen2.5-0.5B" --local-dir "models/Qwen2.5-0.5B" --local-dir-use-symlinks False
```

### 2.2 ä¸‹è½½æ•°æ®

æœ¬é¡¹ç›®é»˜è®¤ä½¿ç”¨ **Industrial and Scientific** æ•°æ®é›†ã€‚è¯·ä» [UCSD Amazon Data](https://nijianmo.github.io/amazon/index.html) ä¸‹è½½ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶å¹¶æ”¾å…¥ `data/raw/`ï¼š

  * `Industrial_and_Scientific_5.json.gz`
  * `meta_Industrial_and_Scientific.json.gz`

-----

## ğŸš€ 3. è¿è¡Œå…¨æµç¨‹ (Step-by-Step Pipeline)

è¯·æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚æ‰€æœ‰å‘½ä»¤å‡å·²è®¾è®¡ä¸ºå•è¡Œæ‰§è¡Œã€‚

### Step 1: æ•°æ®æ¸…æ´— (Data Processing)

è§£æåŸå§‹æ•°æ®ï¼Œæ‰§è¡Œ K-Core è¿‡æ»¤ï¼Œå¹¶æŒ‰ **2017.10-2018.11** æ—¶é—´çª—å£åˆ‡åˆ†æ•°æ®ã€‚

```bash
python tool/amazon18_data_process.py --dataset Industrial_and_Scientific --reviews_file ./data/raw/Industrial_and_Scientific_5.json --metadata_file ./data/raw/meta_Industrial_and_Scientific.json --user_k 5 --item_k 5 --st_year 2017 --st_month 10 --ed_year 2018 --ed_month 11 --output_path ./data/processed     
```

### Step 2: ç”Ÿæˆè¯­ä¹‰ ID (Semantic IDs)

ä½¿ç”¨ Qwen æå–å•†å“æ ‡é¢˜å‘é‡ï¼Œå¹¶é€šè¿‡å±‚çº§èšç±»ç”Ÿæˆ 3 å±‚è¯­ä¹‰ IDã€‚

```bash
python tool/amazon_text2emb.py --dataset Industrial_and_Scientific --root ./data/processed/Industrial_and_Scientific --plm_name qwen --plm_checkpoint "./models/Qwen2.5-0.5B"
```
```bash
python RQ/rqkmeans_faiss.py --dataset Industrial_and_Scientific --data_path data/processed/Industrial_and_Scientific/embeddings/Industrial_and_Scientific.emb-qwen-td.npy
```
### Step 3: æå–ååŒä¿¡å· (Collaborative Signals)

è®­ç»ƒ LightGCN æ¨¡å‹ä»¥æ•è·ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶å¯¼å‡ºç‰©å“ååŒå‘é‡ã€‚

```bash
python CF/train_lightgcn.py   --dataset "Industrial_and_Scientific"   --data_dir "./data/processed"   --output_path "./data/processed/Industrial_and_Scientific/lightgcn_emb.npy"
```

### Step 4: ç”ŸæˆååŒæç¤º (Generate Hints)

åŸºäº LightGCN å‘é‡æ£€ç´¢æ¯ä¸ªç‰©å“çš„ Top-K äº’è¡¥é‚»å±…ï¼Œç”Ÿæˆæç¤ºå­—å…¸ã€‚

```bash
python tool/gen_hints.py   --cf_emb "./data/processed/Industrial_and_Scientific/lightgcn_emb.npy"   --sem_idx "./data/processed/Industrial_and_Scientific/Industrial_and_Scientific.index.json"   --out "./data/processed/Industrial_and_Scientific/cf_hints.json"
```

### Step 5: å‡†å¤‡ SFT æ•°æ® (Prepare Data)

ç”Ÿæˆç²¾ç®€ç‰ˆï¼ˆKeep Longest Onlyï¼‰çš„ä¸»ä»»åŠ¡è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨ç”Ÿæˆæ—¶æ³¨å…¥ Hintã€‚

```bash
python tool/convert_dataset.py   --dataset_name Industrial_and_Scientific   --data_dir ./data/processed/Industrial_and_Scientific   --output_dir ./data/sft_ready  --keep_longest_only   --hints_file "./data/processed/Industrial_and_Scientific/cf_hints.json"
```

### Step 6: SFT è®­ç»ƒ (Training)

å¯åŠ¨å¤šä»»åŠ¡æ··åˆè®­ç»ƒã€‚

  * **æ•°æ®æ„æˆ**ï¼šä¸»ä»»åŠ¡ (5760æ¡) + 4ä¸ªè¾…åŠ©ä»»åŠ¡ (å„é‡‡æ ·5760æ¡) â‰ˆ 2.88ä¸‡æ¡æ•°æ®ã€‚
  * **æœºåˆ¶**ï¼šä¸»ä»»åŠ¡å¯ç”¨ **Hint Dropout (p=0.3)**ï¼Œè®­ç»ƒ 10 Epochs (çº¦ 1.8ä¸‡æ­¥)ã€‚

<!-- end list -->

```bash
python GR/sft.py   --category "Industrial_and_Scientific"   --output_dir "./output/sft_hints"   --base_model "./models/Qwen2.5-0.5B"   --train_file "./data/sft_ready/train/Industrial_and_Scientific_5_2016-10-2018-11.csv"   --eval_file "./data/sft_ready/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv"   --sid_index_path "./data/processed/Industrial_and_Scientific/Industrial_and_Scientific.index.json"   --item_meta_path "./data/processed/Industrial_and_Scientific/Industrial_and_Scientific.item.json"   --learning_rate 2e-5   --micro_batch_size 8   --batch_size 16   --num_epochs 10   --cutoff_len 1024 --cf_hints_path data/processed/Industrial_and_Scientific/cf_hints.json
```

### Step 7: è¯„ä¼° (Evaluation)

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œ Beam Search æ¨ç†ï¼Œå¹¶è®¡ç®— HR@K å’Œ NDCG@Kã€‚
*(æ³¨ï¼šæ¨ç†æ—¶è‡ªåŠ¨ä¿ç•™å®Œæ•´ Hint)*

**ç”Ÿæˆé¢„æµ‹ç»“æœï¼š**

```bash
python tool/evaluate.py   --category "Industrial_and_Scientific"   --base_model "./output/sft/final_checkpoint"   --test_data_path "./data/sft_ready/test/Industrial_and_Scientific_5_2016-10-2018-11.csv"   --info_file "./data/sft_ready/info/Industrial_and_Scientific_5_2016-10-2018-11.txt"   --result_json_data "./output/eval_final.json"   --num_beams 20   --cf_hints_path "./data/processed/Industrial_and_Scientific/cf_hints.json"
```

**è®¡ç®—æŒ‡æ ‡ï¼š**

```bash
python tool/calc.py --file ./output/eval_result.json
```

-----

## ğŸ“Š å®éªŒå¯¹ç…§

| Experiment | Configuration | Hint Strategy |
| :--- | :--- | :--- |
| **Baseline-0.7b** | åŸç‰ˆ0.7bå¤ç° | æ—  Hint |
| **Baseline** | åŸç‰ˆçš„ç»“æœ | æ— hint |
| **miniGR-CF** | **é‡‡ç”¨qwen2.5-0.7bLightGCN å¢å¼ºhints** | **Train: Dropout(0.3) & Clean Target / Test: Full Hint** |

## ğŸ”– Citation & Acknowledgement

æœ¬é¡¹ç›®ä¸»è¦åŸºäºä»¥ä¸‹ä¼˜ç§€å¼€æºå·¥ä½œè¿›è¡Œæ”¹è¿›ï¼š

  * **MiniOneRec**: An Open-Source Framework for Scaling Generative Recommendation.

      * GitHub: [https://github.com/Isheng-Z/MiniOneRec](https://github.com/Isheng-Z/MiniOneRec)
      * Paper: [arXiv:2510.24431](https://arxiv.org/abs/2510.24431)

  * **LightGCN**: Simplifying and Powering Graph Convolution Network for Recommendation.

      * Paper: [SIGIR 2020](https://arxiv.org/abs/2002.02126)

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æœ¬é¡¹ç›®æˆ–åŸå§‹ MiniOneRec ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{MiniOneRec,
  title={MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation},
  author={Xiaoyu Kong and Leheng Sheng and Junfei Tan and Yuxin Chen and Jiancan Wu and An Zhang and Xiang Wang and Xiangnan He},
  year={2025},
  eprint={2510.24431},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}
```

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ [MiniOneRec Team](https://github.com/AkaliKong/MiniOneRec/issues) æä¾›çš„ä»£ç åŸºç¡€å’Œæ•°æ®å¤„ç†è„šæœ¬ã€‚æœ¬é¡¹ç›®çš„æ ¸å¿ƒæ¶æ„ï¼ˆSFT å¤šä»»åŠ¡è®­ç»ƒã€RQ-kmeans ID ç”Ÿæˆï¼‰å‡å¤ç”¨äºè¯¥ä»“åº“ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¢åŠ äº†ååŒä¿¡å·å¢å¼ºæ¨¡å—ã€‚
