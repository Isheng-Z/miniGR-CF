import torch
import torch.nn as nn
from transformers import PreTrainedModel


class VeRALinear(nn.Module):
    def __init__(self, base_layer, rank, device=None, dtype=None):
        super().__init__()
        self.base_layer = base_layer
        self.rank = rank

        # å…±äº«çŸ©é˜µ (Frozen)
        self.S = nn.Parameter(torch.randn(base_layer.in_features, rank, device=device, dtype=dtype) / rank ** 0.5,
                              requires_grad=False)
        self.T = nn.Parameter(torch.zeros(rank, base_layer.out_features, device=device, dtype=dtype),
                              requires_grad=False)

        # ä¸´æ—¶å­˜å‚¨å½“å‰ Batch çš„ User Vectors (ç”± QwenVeRA æ³¨å…¥)
        self.current_user_vectors = None

    def forward(self, x):
        # 1. Base Forward (Frozen)
        base_out = self.base_layer(x)  # [Batch, Seq, Out]

        # 2. VeRA Forward (Parallel)
        if self.current_user_vectors is None:
            return base_out

        # user_vectors: [Batch, 2 * rank] -> split to b, d
        # è¿™é‡Œçš„ Batch ç»´åº¦å¿…é¡»å’Œ x çš„ Batch ç»´åº¦å¯¹é½
        vec = self.current_user_vectors

        # æ ¡éªŒ Batch Size (å¤„ç† GRPO é‡‡æ ·æ—¶çš„ç»´åº¦æ‰©å±•)
        if vec.shape[0] != x.shape[0]:
            # å¦‚æœè¾“å…¥ x æ˜¯ user_vec çš„ G å€ (å› ä¸ºç”Ÿæˆäº† G ä¸ªæ ·æœ¬)
            ratio = x.shape[0] // vec.shape[0]
            vec = vec.repeat_interleave(ratio, dim=0)

        b_vec, d_vec = torch.chunk(vec, 2, dim=-1)  # [Batch, Rank]

        # æŠ•å½±åˆ°ä½ç§©ç©ºé—´ [Batch, Seq, Rank]
        low_rank = x @ self.S

        # å…³é”®ï¼šå¹¶è¡Œæ³¨å…¥ä¸ªæ€§åŒ–å‚æ•°
        # [Batch, Seq, Rank] * [Batch, 1, Rank] -> Broadcasting
        low_rank = low_rank * b_vec.unsqueeze(1) * d_vec.unsqueeze(1)

        # æŠ•å½±å›è¾“å‡ºç©ºé—´
        delta_out = low_rank @ self.T

        return base_out + delta_out


class QwenVeRA(nn.Module):
    def __init__(self, base_model, num_users, rank=256):
        super().__init__()
        self.base_model = base_model
        self.rank = rank
        self.num_users = num_users

        # å†»ç»“åŸºåº§
        for param in self.base_model.parameters():
            param.requires_grad = False

        # æ›¿æ¢ Linear å±‚
        self.vera_layers = nn.ModuleList()
        # é’ˆå¯¹ Qwen2.5 çš„æ¨¡å—å
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

        for name, module in self.base_model.named_modules():
            if any(t in name.split('.')[-1] for t in target_modules) and isinstance(module, nn.Linear):
                parent_name = ".".join(name.split(".")[:-1])
                child_name = name.split(".")[-1]
                parent = self.base_model.get_submodule(parent_name)

                vera_layer = VeRALinear(module, rank, device=module.weight.device, dtype=module.weight.dtype)
                setattr(parent, child_name, vera_layer)
                self.vera_layers.append(vera_layer)

        # ç”¨æˆ·ç‹¬ç«‹å‚æ•° (Trainable) - å­˜æ˜¾å­˜
        # ç»´åº¦: [Users, Layers * 2 * Rank]
        total_dim = len(self.vera_layers) * (2 * rank)
        print(f"ğŸ§  Allocating User Embeddings: [{num_users}, {total_dim}]")
        self.user_embeddings = nn.Embedding(num_users, total_dim).to(base_model.device)
        # åˆå§‹åŒ–
        nn.init.normal_(self.user_embeddings.weight, std=0.01)
        self.user_embeddings.weight.requires_grad = True

    def forward(self, input_ids, user_ids, **kwargs):
        # 1. æŸ¥è¡¨è·å–å½“å‰ Batch çš„å‚æ•° [Batch, Total_Dim]
        all_vecs = self.user_embeddings(user_ids)

        # 2. æ‹†åˆ†åˆ°æ¯ä¸€å±‚
        batch_size = user_ids.size(0)
        layer_dim = 2 * self.rank
        all_vecs = all_vecs.view(batch_size, len(self.vera_layers), layer_dim)

        # 3. æ³¨å…¥åˆ°å„ä¸ª Layer ä¸­ (State Injection)
        for i, layer in enumerate(self.vera_layers):
            layer.current_user_vectors = all_vecs[:, i, :]

        # 4. æ‰§è¡ŒåŸºåº§çš„å‰å‘ä¼ æ’­ (ä¼šè§¦å‘ VeRALinear.forward)
        return self.base_model(input_ids, **kwargs)

    def generate(self, input_ids, user_ids, **kwargs):
        # ç±»ä¼¼äº forwardï¼Œå…ˆæ³¨å…¥å‚æ•°ï¼Œå†è°ƒç”¨ generate
        # æ³¨æ„ï¼šè¿™é‡Œ user_ids åªéœ€è¦ä¼ å…¥åŸå§‹ Batch çš„ ID
        # generate å†…éƒ¨æ‰©å±• input_ids æ—¶ï¼ŒVeRALinear ä¼šè‡ªåŠ¨ repeat_interleave
        with torch.no_grad():
            # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ³¨å…¥ current_user_vectors
            self.forward(input_ids, user_ids)

        return self.base_model.generate(input_ids=input_ids, **kwargs)

    def save_vera(self, path):
        torch.save(self.user_embeddings.state_dict(), path)